{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data/ptb.zip\n",
      "   creating: data/ptb/\n",
      "  inflating: data/ptb/reader.py      \n",
      "   creating: data/__MACOSX/\n",
      "   creating: data/__MACOSX/ptb/\n",
      "  inflating: data/__MACOSX/ptb/._reader.py  \n",
      "  inflating: data/__MACOSX/._ptb     \n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!wget -q -O data/ptb.zip https://ibm.box.com/shared/static/z2yvmhbskc45xd2a9a4kkn6hg4g4kj5r.zip\n",
    "!unzip -o data/ptb.zip -d data\n",
    "!cp data/ptb/reader.py .\n",
    "\n",
    "import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-12-27 10:44:39--  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
      "Resolving www.fit.vutbr.cz (www.fit.vutbr.cz)... 147.229.9.23, 2001:67c:1220:809::93e5:917\n",
      "Connecting to www.fit.vutbr.cz (www.fit.vutbr.cz)|147.229.9.23|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 34869662 (33M) [application/x-gtar]\n",
      "Saving to: ‘simple-examples.tgz’\n",
      "\n",
      "simple-examples.tgz 100%[===================>]  33.25M  4.04MB/s    in 9.6s    \n",
      "\n",
      "2019-12-27 10:44:49 (3.45 MB/s) - ‘simple-examples.tgz’ saved [34869662/34869662]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz \n",
    "!tar xzf simple-examples.tgz -C data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Initial weight scale\n",
    "init_scale = 0.1\n",
    "#Initial learning rate\n",
    "learning_rate = 1.0\n",
    "#Maximum permissible norm for the gradient (For gradient clipping -- another measure against Exploding Gradients)\n",
    "max_grad_norm = 5\n",
    "#The number of layers in our model\n",
    "num_layers = 2\n",
    "#The total number of recurrence steps, also known as the number of layers when our RNN is \"unfolded\"\n",
    "num_steps = 20\n",
    "#The number of processing units (neurons) in the hidden layers\n",
    "hidden_size_l1 = 256\n",
    "hidden_size_l2 = 128\n",
    "#The maximum number of epochs trained with the initial learning rate\n",
    "max_epoch_decay_lr = 4\n",
    "#The total number of epochs in training\n",
    "max_epoch = 15\n",
    "#The probability for keeping data in the Dropout Layer (This is an optimization, but is outside our scope for this notebook!)\n",
    "#At 1, we ignore the Dropout Layer wrapping.\n",
    "keep_prob = 1\n",
    "#The decay for the learning rate\n",
    "decay = 0.5\n",
    "#The size for each batch of data\n",
    "batch_size = 60\n",
    "#The size of our vocabulary\n",
    "vocab_size = 10000\n",
    "embeding_vector_size = 200\n",
    "#Training flag to separate training from testing\n",
    "is_training = 1\n",
    "#Data directory for our dataset\n",
    "data_dir = \"data/simple-examples/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "session = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Reads the data and separates it into training data, validation data and testing data\n",
    "raw_data = reader.ptb_raw_data(data_dir)\n",
    "train_data, valid_data, test_data, vocab, word_to_id = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "929589"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food', 'ssangyong', 'swapo', 'wachter', '<eos>', 'pierre', '<unk>', 'N', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', 'N', '<eos>', 'mr.', '<unk>', 'is', 'chairman', 'of', '<unk>', 'n.v.', 'the', 'dutch', 'publishing', 'group', '<eos>', 'rudolph', '<unk>', 'N', 'years', 'old', 'and', 'former', 'chairman', 'of', 'consolidated', 'gold', 'fields', 'plc', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'british', 'industrial', 'conglomerate', '<eos>', 'a', 'form', 'of', 'asbestos', 'once', 'used', 'to', 'make', 'kent', 'cigarette', 'filters', 'has', 'caused', 'a', 'high', 'percentage', 'of', 'cancer', 'deaths', 'among', 'a', 'group', 'of']\n"
     ]
    }
   ],
   "source": [
    "def id_to_word(id_list):\n",
    "    line = []\n",
    "    for w in id_list:\n",
    "        for word, wid in word_to_id.items():\n",
    "            if wid == w:\n",
    "                line.append(word)\n",
    "    return line            \n",
    "                \n",
    "\n",
    "print(id_to_word(train_data[0:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Lets just read one mini-batch now and feed our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "itera = reader.ptb_iterator(train_data, batch_size, num_steps)\n",
    "first_touple = itera.__next__()\n",
    "x = first_touple[0]\n",
    "y = first_touple[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 20)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Lets look at 3 sentences of our input x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9970, 9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984,\n",
       "        9986, 9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995],\n",
       "       [ 901,   33, 3361,    8, 1279,  437,  597,    6,  261, 4276, 1089,\n",
       "           8, 2836,    2,  269,    4, 5526,  241,   13, 2420],\n",
       "       [2654,    6,  334, 2886,    4,    1,  233,  711,  834,   11,  130,\n",
       "         123,    7,  514,    2,   63,   10,  514,    8,  605]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "we define 2 place holders to feed them with mini-batchs, that is x and y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "_input_data = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]\n",
    "_targets = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Lets define a dictionary, and use it later to feed the placeholders with our first mini-batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "feed_dict = {_input_data:x, _targets:y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "For example, we can use it to feed <code>\\_input\\_data</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9970, 9971, 9972, ..., 9993, 9994, 9995],\n",
       "       [ 901,   33, 3361, ...,  241,   13, 2420],\n",
       "       [2654,    6,  334, ...,  514,    8,  605],\n",
       "       ...,\n",
       "       [7831,   36, 1678, ...,    4, 4558,  157],\n",
       "       [  59, 2070, 2433, ...,  400,    1, 1173],\n",
       "       [2097,    3,    2, ..., 2043,   23,    1]], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(_input_data, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this step, we create the stacked LSTM, which is a 2 layer LSTM network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "lstm_cell_l1 = tf.contrib.rnn.BasicLSTMCell(hidden_size_l1, forget_bias=0.0)\n",
    "lstm_cell_l2 = tf.contrib.rnn.BasicLSTMCell(hidden_size_l2, forget_bias=0.0)\n",
    "stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell_l1, lstm_cell_l2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Also, we initialize the states of the nework:\n",
    "\n",
    "<h4>_initial_state</h4>\n",
    "\n",
    "For each LCTM, there are 2 state matrices, c\\_state and m\\_state.  c_state and m_state represent \"Memory State\" and \"Cell State\". Each hidden layer, has a vector of size 30, which keeps the states. so, for 200 hidden units in each LSTM, we have a matrix of size [30x200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState/zeros:0' shape=(60, 256) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(60, 256) dtype=float32>),\n",
       " LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState_1/zeros:0' shape=(60, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState_1/zeros_1:0' shape=(60, 128) dtype=float32>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_initial_state = stacked_lstm.zero_state(batch_size, tf.float32)\n",
    "_initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Lets look at the states, though they are all zero for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LSTMStateTuple(c=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), h=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)),\n",
       " LSTMStateTuple(c=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), h=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(_initial_state, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>Embeddings</h3>\n",
    "We have to convert the words in our dataset to vectors of numbers. The traditional approach is to use one-hot encoding method that is usually used for converting categorical values to numerical values. However, One-hot encoded vectors are high-dimensional, sparse and in a big dataset, computationally inefficient. So, we use word2vec approach. It is, in fact, a layer in our LSTM network, where the word IDs will be represented as a dense representation before feeding to the LSTM. \n",
    "\n",
    "The embedded vectors also get updated during the training process of the deep neural network.\n",
    "We create the embeddings for our input data. <b>embedding_vocab</b> is matrix of [10000x200] for all 10000 unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "embedding_vocab = tf.get_variable(\"embedding_vocab\", [vocab_size, embeding_vector_size])  #[10000x200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets initialize the <code>embedding_words</code> variable with random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01140717,  0.02023567, -0.01408729, ...,  0.02321866,\n",
       "        -0.02281097,  0.0064531 ],\n",
       "       [-0.01239462, -0.01340416,  0.01333335, ..., -0.01969419,\n",
       "         0.00411844, -0.0067297 ],\n",
       "       [ 0.0042109 ,  0.01001752, -0.00874475, ..., -0.00572756,\n",
       "         0.01278892,  0.00889745],\n",
       "       ...,\n",
       "       [-0.02324303, -0.01835205, -0.0232351 , ..., -0.01234556,\n",
       "        -0.00086483,  0.01502647],\n",
       "       [-0.00759253, -0.01854469,  0.0218081 , ...,  0.00881909,\n",
       "         0.00319346, -0.01668934],\n",
       "       [-0.0044563 ,  0.01767663,  0.0168043 , ...,  0.0110224 ,\n",
       "        -0.00375866, -0.0195694 ]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(embedding_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<b>embedding_lookup()</b> finds the embedded values for our batch of 30x20 words. It  goes to each row of <code>input_data</code>, and for each word in the row/sentence, finds the correspond vector in <code>embedding_dic<code>. <br>\n",
    "It creates a [30x20x200] tensor, so, the first element of <b>inputs</b> (the first sentence), is a matrix of 20x200, which each row of it, is vector representing a word in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup:0' shape=(60, 20, 200) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define where to get the data for our embeddings from\n",
    "inputs = tf.nn.embedding_lookup(embedding_vocab, _input_data)  #shape=(30, 20, 200) \n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01662669, -0.00851611,  0.01330983, ...,  0.00729107,\n",
       "        -0.0086226 , -0.02140759],\n",
       "       [-0.01220327, -0.01397082,  0.00352248, ...,  0.01791978,\n",
       "         0.0130741 ,  0.01054953],\n",
       "       [-0.0205128 , -0.00927834,  0.00490582, ...,  0.0176493 ,\n",
       "         0.01509868,  0.01343441],\n",
       "       ...,\n",
       "       [-0.00282781,  0.01959827, -0.01643595, ..., -0.0207481 ,\n",
       "        -0.01513887,  0.0176754 ],\n",
       "       [ 0.00578189, -0.0148677 ,  0.00959211, ...,  0.00093682,\n",
       "         0.00683254,  0.0033809 ],\n",
       "       [ 0.00332061,  0.01277157,  0.01495042, ..., -0.02184252,\n",
       "        -0.00828377,  0.00992631]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(inputs[0], feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>Constructing Recurrent Neural Networks</h3>\n",
    "<b>tf.nn.dynamic_rnn()</b> creates a recurrent neural network using <b>stacked_lstm</b>. \n",
    "\n",
    "The input should be a Tensor of shape: [batch_size, max_time, embedding_vector_size], in our case it would be (30, 20, 200)\n",
    "\n",
    "This method, returns a pair (outputs, new_state) where:\n",
    "<ul>\n",
    "    <li><b>outputs</b>: is a length T list of outputs (one for each input), or a nested tuple of such elements.</li>\n",
    "    <li><b>new_state</b>: is the final state.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outputs, new_state =  tf.nn.dynamic_rnn(stacked_lstm, inputs, initial_state=_initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "so, lets look at the outputs. The output of the stackedLSTM comes from 200 hidden_layer, and in each time step(=20), one of them get activated. we use the linear activation to map the 200 hidden layer to a [?x10 matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'rnn/transpose_1:0' shape=(60, 20, 128) dtype=float32>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.71888416e-04, -1.90442035e-04, -1.21335215e-04, ...,\n",
       "         2.21405480e-05,  1.08889930e-04,  5.06063749e-04],\n",
       "       [-4.61957592e-04, -2.51115533e-04,  3.18524049e-04, ...,\n",
       "         7.01285608e-04,  3.02254222e-04,  7.67858874e-04],\n",
       "       [-4.44965379e-04, -5.86587936e-04,  5.11239341e-04, ...,\n",
       "         5.55076229e-04,  7.23287172e-04,  6.18352322e-04],\n",
       "       ...,\n",
       "       [ 6.12409320e-04, -3.98175529e-04,  1.65266720e-05, ...,\n",
       "         9.90393106e-04, -7.42864737e-04, -1.90373365e-04],\n",
       "       [ 1.09831581e-03, -8.58814514e-04, -5.45308867e-04, ...,\n",
       "         9.48285277e-04, -6.45909866e-04,  6.32744486e-05],\n",
       "       [ 8.09730322e-04, -5.29670913e-04, -6.29357179e-04, ...,\n",
       "         1.45526428e-03, -8.19438777e-04,  9.04873887e-05]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(outputs[0], feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "we need to flatten the outputs to be able to connect it softmax layer. Lets reshape the output tensor from  [30 x 20 x 200] to [600 x 200].\n",
    "\n",
    "<b>Notice:</b> Imagine our output is 3-d tensor as following (of course each <code>sen_x_word_y</code> is a an embedded vector by itself): \n",
    "<ul>\n",
    "    <li>sentence 1: [[sen1word1], [sen1word2], [sen1word3], ..., [sen1word20]]</li> \n",
    "    <li>sentence 2: [[sen2word1], [sen2word2], [sen2word3], ..., [sen2word20]]</li>   \n",
    "    <li>sentence 3: [[sen3word1], [sen3word2], [sen3word3], ..., [sen3word20]]</li>  \n",
    "    <li>...  </li>\n",
    "    <li>sentence 30: [[sen30word1], [sen30word2], [sen30word3], ..., [sen30word20]]</li>   \n",
    "</ul>\n",
    "Now, the flatten would convert this 3-dim tensor to:\n",
    "\n",
    "[ [sen1word1], [sen1word2], [sen1word3], ..., [sen1word20],[sen2word1], [sen2word2], [sen2word3], ..., [sen2word20], ..., [sen30word20] ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(1200, 128) dtype=float32>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tf.reshape(outputs, [-1, hidden_size_l2])\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>logistic unit</h3>\n",
    "Now, we create a logistic unit to return the probability of the output word in our vocabulary with 1000 words. \n",
    "\n",
    "$$Softmax = [600 \\times 200] * [200 \\times 1000] + [1 \\times 1000] \\Longrightarrow [600 \\times 1000]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "softmax_w = tf.get_variable(\"softmax_w\", [hidden_size_l2, vocab_size]) #[200x1000]\n",
    "softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) #[1x1000]\n",
    "logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "prob = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the probability of observing words for t=0 to t=20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the output:  (1200, 10000)\n",
      "The probability of observing words in t=0 to t=20 [[1.01532700e-04 1.00643199e-04 9.89859182e-05 ... 1.01271857e-04\n",
      "  9.84303551e-05 1.00540215e-04]\n",
      " [1.01536200e-04 1.00645731e-04 9.89801410e-05 ... 1.01274352e-04\n",
      "  9.84306753e-05 1.00541707e-04]\n",
      " [1.01547368e-04 1.00649733e-04 9.89855689e-05 ... 1.01275567e-04\n",
      "  9.84364378e-05 1.00542115e-04]\n",
      " ...\n",
      " [1.01549078e-04 1.00623103e-04 9.89786859e-05 ... 1.01270205e-04\n",
      "  9.84297803e-05 1.00543286e-04]\n",
      " [1.01543352e-04 1.00628902e-04 9.89790424e-05 ... 1.01269208e-04\n",
      "  9.84296930e-05 1.00541605e-04]\n",
      " [1.01558013e-04 1.00641366e-04 9.89852924e-05 ... 1.01264923e-04\n",
      "  9.84261424e-05 1.00538244e-04]]\n"
     ]
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "output_words_prob = session.run(prob, feed_dict)\n",
    "print(\"shape of the output: \", output_words_prob.shape)\n",
    "print(\"The probability of observing words in t=0 to t=20\", output_words_prob[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>Prediction</h3>\n",
    "What is the word correspond to the probability output? Lets use the maximum probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  46,   46, 4687, 2919, 6467, 6467,  352,  352, 3561, 8549, 3868,\n",
       "       3868, 5674, 7517, 7517, 2139, 9659, 2077, 2077, 7246])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(output_words_prob[0:20], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "So, what is the ground truth for the first word of first sentence? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984, 9986,\n",
       "       9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995, 9996], dtype=int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Also, you can get it from target tensor, if you want to find the embedding vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984, 9986,\n",
       "       9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995, 9996], dtype=int32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ = session.run(_targets, feed_dict) \n",
    "targ[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How similar the predicted words are to the target words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h4>Objective function</h4>\n",
    "\n",
    "Now we have to define our objective function, to calculate the similarity of predicted values to ground truth, and then, penalize the model with the error. Our objective is to minimize loss function, that is, to minimize the average negative log probability of the target words:\n",
    "\n",
    "$$\\text{loss} = -\\frac{1}{N}\\sum_{i=1}^{N} \\ln p_{\\text{target}_i}$$\n",
    "\n",
    "This function is already implemented and available in TensorFlow through <b>sequence_loss_by_example</b>. It calculates the weighted cross-entropy loss for <b>logits</b> and the <b>target</b> sequence.  \n",
    "\n",
    "The arguments of this function are:  \n",
    "<ul>\n",
    "    <li>logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].</li>  \n",
    "    <li>targets: List of 1D batch-sized int32 Tensors of the same length as logits.</li>   \n",
    "    <li>weights: List of 1D batch-sized float-Tensors of the same length as logits.</li> \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(_targets, [-1])],[tf.ones([batch_size * num_steps])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "loss is a 1D batch-sized float Tensor [600x1]: The log-perplexity for each sequence. Lets look at the first 10 values of loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.204408, 9.224222, 9.202195, 9.224459, 9.195793, 9.21846 ,\n",
       "       9.19586 , 9.225675, 9.224486, 9.205398], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(loss, feed_dict)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define loss as average of the losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184.19095"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = tf.reduce_sum(loss) / batch_size\n",
    "session.run(tf.global_variables_initializer())\n",
    "session.run(cost, feed_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h4>1. Define Optimizer</h4>\n",
    "\n",
    "<b>GradientDescentOptimizer</b> constructs a new gradient descent optimizer. Later, we use constructed <b>optimizer</b> to compute gradients for a loss and apply gradients to variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a variable for the learning rate\n",
    "lr = tf.Variable(0.0, trainable=False)\n",
    "# Create the gradient descent optimizer with our learning rate\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "\n",
    "<h4>2. Trainable Variables</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Defining a variable, if you passed <i>trainable=True</i>, the variable constructor automatically adds new variables to the graph collection <b>GraphKeys.TRAINABLE_VARIABLES</b>. Now, using <i>tf.trainable_variables()</i> you can get all variables created with <b>trainable=True</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'embedding_vocab:0' shape=(10000, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(456, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(384, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'softmax_w:0' shape=(128, 10000) dtype=float32_ref>,\n",
       " <tf.Variable 'softmax_b:0' shape=(10000,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\n",
    "tvars = tf.trainable_variables()\n",
    "tvars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Note: we can find the name and scope of all variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['embedding_vocab:0',\n",
       " 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0',\n",
       " 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0',\n",
       " 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0',\n",
       " 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0',\n",
       " 'softmax_w:0',\n",
       " 'softmax_b:0']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tvars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h4>3. Calculate the gradients based on the loss function</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h4>Gradient</h4>:\n",
    "The gradient of a function is the slope of its derivative (line), or in other words, the rate of change of a function. It's a vector (a direction to move) that points in the direction of greatest increase of the function, and calculated by the <b>derivative</b> operation.\n",
    "\n",
    "First lets recall the gradient function using an toy example:\n",
    "$$ z = \\left(2x^2 + 3xy\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_x = tf.placeholder(tf.float32)\n",
    "var_y = tf.placeholder(tf.float32) \n",
    "func_test = 2.0 * var_x * var_x + 3.0 * var_x * var_y\n",
    "session.run(tf.global_variables_initializer())\n",
    "session.run(func_test, {var_x:1.0,var_y:2.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The <b>tf.gradients()</b> function allows you to compute the symbolic gradient of one tensor with respect to one or more other tensors—including variables. <b>tf.gradients(func, xs)</b> constructs symbolic partial derivatives of sum of <b>func</b> w.r.t. <i>x</i> in <b>xs</b>. \n",
    "\n",
    "Now, lets look at the derivitive w.r.t. <b>var_x</b>:\n",
    "$$ \\frac{\\partial \\:}{\\partial \\:x}\\left(2x^2 + 3xy\\right) = 4x + 3y $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.0]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_grad = tf.gradients(func_test, [var_x])\n",
    "session.run(var_grad, {var_x:1.0,var_y:2.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "the derivative w.r.t. <b>var_y</b>:\n",
    "$$ \\frac{\\partial \\:}{\\partial \\:x}\\left(2x^2 + 3xy\\right) = 3x $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.0]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_grad = tf.gradients(func_test, [var_y])\n",
    "session.run(var_grad, {var_x:1.0, var_y:2.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, we can look at gradients w.r.t all variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.framework.ops.IndexedSlices at 0x7f5c6807eeb8>,\n",
       " <tf.Tensor 'gradients_2/rnn/while/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/MatMul/Enter_grad/b_acc_3:0' shape=(456, 1024) dtype=float32>,\n",
       " <tf.Tensor 'gradients_2/rnn/while/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/BiasAdd/Enter_grad/b_acc_3:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Tensor 'gradients_2/rnn/while/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/MatMul/Enter_grad/b_acc_3:0' shape=(384, 512) dtype=float32>,\n",
       " <tf.Tensor 'gradients_2/rnn/while/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/BiasAdd/Enter_grad/b_acc_3:0' shape=(512,) dtype=float32>,\n",
       " <tf.Tensor 'gradients_2/MatMul_grad/MatMul_1:0' shape=(128, 10000) dtype=float32>,\n",
       " <tf.Tensor 'gradients_2/add_grad/Reshape_1:0' shape=(10000,) dtype=float32>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gradients(cost, tvars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grad_t_list = tf.gradients(cost, tvars)\n",
    "#sess.run(grad_t_list,feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.framework.ops.IndexedSlices at 0x7f5c680a14a8>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_1:0' shape=(456, 1024) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_2:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_3:0' shape=(384, 512) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_4:0' shape=(512,) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_5:0' shape=(128, 10000) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_6:0' shape=(10000,) dtype=float32>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the gradient clipping threshold\n",
    "grads, _ = tf.clip_by_global_norm(grad_t_list, max_grad_norm)\n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[IndexedSlicesValue(values=array([[-2.06744880e-05,  6.64760682e-06,  5.72348654e-06, ...,\n",
       "         -7.17892681e-06,  6.39652080e-06, -1.54186273e-06],\n",
       "        [-1.59200627e-05,  2.77802678e-06,  6.61823606e-07, ...,\n",
       "         -3.91067124e-06,  8.30206409e-06, -1.75804166e-06],\n",
       "        [-2.84447560e-05,  5.82286202e-06, -3.61220259e-07, ...,\n",
       "         -2.71707586e-06,  1.12787102e-05, -1.86755574e-06],\n",
       "        ...,\n",
       "        [ 1.46565776e-06,  4.07864763e-06, -1.85293447e-05, ...,\n",
       "         -8.35601156e-07, -5.48072376e-06,  2.27930468e-06],\n",
       "        [ 5.22765504e-06, -1.01445255e-06, -1.20398945e-05, ...,\n",
       "          1.34440950e-06, -2.39876982e-07,  1.15070748e-07],\n",
       "        [ 4.00256113e-06, -1.54685074e-06, -4.91942501e-06, ...,\n",
       "         -8.29346547e-08, -1.01783417e-06,  3.10083720e-07]], dtype=float32), indices=array([9970, 9971, 9972, ..., 2043,   23,    1], dtype=int32), dense_shape=array([10000,   200], dtype=int32)),\n",
       " array([[ 1.9072011e-09, -4.0107182e-08, -6.1586021e-09, ...,\n",
       "         -1.4069958e-08, -6.0896119e-09,  2.9068628e-08],\n",
       "        [ 1.4125741e-09,  1.7181687e-08,  3.9424265e-08, ...,\n",
       "          3.6896353e-08, -2.6851179e-08,  1.8982615e-08],\n",
       "        [ 1.9938041e-08, -3.9975060e-08, -2.1341197e-08, ...,\n",
       "         -2.5004768e-08, -1.1599960e-08,  3.3652821e-08],\n",
       "        ...,\n",
       "        [ 1.0222696e-09,  3.7212466e-10,  1.7654307e-09, ...,\n",
       "         -9.6314476e-09,  7.8096096e-10, -3.6796839e-09],\n",
       "        [-9.0747241e-09,  2.9906039e-10,  1.2048242e-09, ...,\n",
       "          6.5681183e-10,  2.3597555e-09,  2.9113417e-10],\n",
       "        [ 3.3102492e-09, -9.8986219e-10,  2.0955013e-09, ...,\n",
       "         -2.3883082e-09,  4.0861989e-09, -5.1226836e-09]], dtype=float32),\n",
       " array([-3.8570879e-06,  3.2759083e-06, -8.3573184e-07, ...,\n",
       "        -5.1715892e-07, -3.2964132e-07, -2.6304174e-06], dtype=float32),\n",
       " array([[-1.2936068e-08,  1.7941698e-09,  2.6189579e-09, ...,\n",
       "          2.8597091e-10,  1.4491247e-09, -5.0442241e-09],\n",
       "        [ 1.7702491e-09, -6.0734209e-09,  1.7507031e-09, ...,\n",
       "          2.3153712e-09,  8.7525187e-09,  2.0297153e-09],\n",
       "        [-9.4641077e-09,  7.5246231e-10,  1.3057755e-09, ...,\n",
       "          8.2437861e-09, -4.5811235e-09, -4.0195789e-09],\n",
       "        ...,\n",
       "        [ 2.7785887e-10,  5.4610416e-10,  8.2605034e-10, ...,\n",
       "         -3.6541885e-09, -1.1851746e-09, -1.6479967e-09],\n",
       "        [-1.1257515e-09,  2.1123103e-09,  1.5751506e-09, ...,\n",
       "         -9.8001418e-10, -7.8156859e-09,  1.2180916e-09],\n",
       "        [ 3.0647234e-09,  1.1930597e-10,  5.9733862e-10, ...,\n",
       "         -6.7180345e-10,  1.8364473e-09,  4.0227461e-09]], dtype=float32),\n",
       " array([-5.38520453e-07,  5.35720119e-07,  1.72445198e-07, -5.58479542e-07,\n",
       "        -7.82637380e-06,  4.04534831e-06,  6.52535982e-06,  6.12108806e-06,\n",
       "         3.57654585e-06, -1.48729384e-06, -6.27853569e-06,  9.38808967e-07,\n",
       "         1.33917104e-06,  3.06307288e-06,  5.62835965e-07, -4.06378877e-06,\n",
       "        -6.36522145e-07, -8.51691709e-07,  8.35782316e-07,  6.39828340e-06,\n",
       "        -7.55110204e-06,  2.24677342e-06,  1.35884841e-06, -2.69283555e-06,\n",
       "         2.47069829e-06, -2.68191366e-06, -2.19865410e-06, -1.18146477e-06,\n",
       "         2.47779076e-06,  2.33734568e-06,  1.83012889e-06, -4.24132168e-06,\n",
       "         8.04209378e-07, -2.38921643e-07, -2.32535558e-06,  9.94734819e-06,\n",
       "         2.38403163e-06, -7.24312440e-06, -6.97487224e-07, -3.26955410e-06,\n",
       "         3.67941357e-06, -1.97309987e-06,  3.31498677e-06, -3.38842511e-07,\n",
       "         2.68788563e-06,  8.08502591e-07,  7.04021488e-07, -2.14927991e-06,\n",
       "        -2.80474524e-06, -1.05358936e-06,  3.09745701e-06,  2.97905808e-06,\n",
       "        -4.47438993e-07,  1.06438656e-05,  4.43859517e-06, -1.43373745e-06,\n",
       "        -1.47896526e-06,  9.47393744e-07,  1.97070926e-06, -2.04280082e-06,\n",
       "         1.92631796e-06, -4.50357174e-06, -1.27360090e-06,  6.07155243e-06,\n",
       "        -5.06591732e-06, -6.65448897e-06, -1.76821516e-06, -1.14304271e-06,\n",
       "         5.90235913e-06, -4.84759539e-06,  1.07333490e-05,  2.61528885e-06,\n",
       "         7.83624898e-07, -1.62660672e-06,  1.12532985e-06, -7.80844118e-07,\n",
       "         6.26075007e-07,  2.13786006e-06,  2.19457888e-06,  5.64976563e-06,\n",
       "         7.28869566e-07, -8.31799412e-07, -4.63505876e-06, -2.29152261e-06,\n",
       "         3.71771944e-06,  3.23126710e-06, -2.53914095e-06, -2.68379858e-06,\n",
       "        -4.29504280e-06, -4.10506254e-06, -1.99823558e-06,  7.14932412e-07,\n",
       "         1.16546119e-06, -6.97739893e-08,  3.88741682e-06,  5.62845287e-07,\n",
       "         1.68383633e-06,  2.76260835e-06, -7.46838964e-07, -3.14547492e-07,\n",
       "        -8.41036012e-07,  1.22029439e-06,  9.88141619e-06,  3.81862833e-07,\n",
       "        -3.14988597e-06, -4.55100826e-06, -9.03629257e-07,  4.29676084e-06,\n",
       "         2.23805023e-06, -1.10638473e-06,  2.97416227e-06,  1.13465433e-08,\n",
       "        -3.06939057e-07, -3.60262611e-08, -1.07332039e-06, -5.01811701e-06,\n",
       "         4.34197227e-06, -1.12478585e-06, -1.32357070e-06,  5.40695737e-06,\n",
       "        -1.32585296e-06,  7.26734925e-06, -8.32104320e-07,  2.57350052e-06,\n",
       "         1.16088620e-06,  2.93835274e-06,  5.15001648e-06, -2.14760666e-06,\n",
       "        -1.46891400e-02,  6.19646162e-05, -3.60374874e-03, -3.24794604e-03,\n",
       "         1.39545975e-02, -2.01894175e-02, -2.06645131e-02, -2.97434330e-02,\n",
       "        -1.20782889e-02,  1.16608944e-02, -2.24932898e-02,  1.29942978e-02,\n",
       "         1.96506381e-02,  1.27727548e-02, -7.95569737e-03,  1.70783810e-02,\n",
       "        -7.80510763e-03,  2.04229653e-02,  1.24241123e-02, -1.76682621e-02,\n",
       "         7.36198621e-03,  1.14970384e-02,  2.78477627e-03,  9.83974338e-03,\n",
       "        -5.27893705e-03, -3.64085147e-03, -8.25959817e-03,  1.59510579e-02,\n",
       "         1.44704152e-02,  7.69404275e-03, -1.58017147e-02, -3.05518359e-02,\n",
       "        -9.05955117e-03, -2.84861145e-03,  3.95046826e-03, -2.57108174e-02,\n",
       "         1.24678640e-02,  1.37998601e-02, -2.72062817e-03, -8.70667212e-03,\n",
       "         1.22137461e-02,  1.64741538e-02, -6.15440262e-03,  5.73992962e-03,\n",
       "         3.85500211e-03, -1.22649577e-02, -2.47549862e-02,  1.74975558e-03,\n",
       "        -2.15358380e-02, -1.58968114e-03,  1.58995632e-02,  6.09630998e-03,\n",
       "        -2.33556889e-03, -2.37245541e-02, -1.26485033e-02, -1.24448668e-02,\n",
       "         2.63077952e-03,  7.37195648e-03,  9.59475804e-03,  4.61765239e-03,\n",
       "        -6.77636918e-03, -1.30897965e-02,  1.64621454e-02,  1.38369435e-02,\n",
       "         3.03072110e-03, -1.55320643e-02,  1.22319870e-02,  3.05661611e-04,\n",
       "         1.23923840e-02,  5.77763282e-03, -3.02152429e-02,  3.95202218e-03,\n",
       "        -5.47407335e-03, -2.67996918e-03, -1.57021321e-02,  2.34624045e-03,\n",
       "        -2.50721239e-02, -3.18749510e-02, -1.83130975e-03,  1.77993756e-02,\n",
       "         1.25118000e-02,  5.70957875e-03, -7.13254511e-03,  1.30755675e-03,\n",
       "         1.24017838e-02, -2.39826273e-03,  1.61351487e-02, -2.32686196e-02,\n",
       "        -8.21948703e-03, -1.68805644e-02,  1.10314810e-03, -1.35089681e-02,\n",
       "        -1.84011254e-02,  6.97187847e-03,  8.71810690e-03, -2.33694855e-02,\n",
       "        -9.66362190e-03, -1.12859122e-02,  1.37116923e-03, -2.11545490e-02,\n",
       "         4.01364407e-03,  2.99169328e-02, -3.26527543e-02, -2.95433623e-04,\n",
       "        -1.39650851e-02,  1.03056654e-02, -1.25138145e-02,  1.62419397e-02,\n",
       "         1.01986900e-02, -3.02283000e-03, -1.33443791e-02,  4.86938562e-03,\n",
       "         2.95812450e-03,  1.75212533e-03,  8.88506602e-03,  1.85659062e-02,\n",
       "        -2.52406057e-02, -2.66766408e-03, -3.63781070e-03, -1.66450944e-02,\n",
       "         1.48385437e-02, -8.48875754e-03, -4.32446692e-03,  2.63787974e-02,\n",
       "        -1.08615952e-02, -1.97876468e-02, -2.77753286e-02,  2.01253202e-02,\n",
       "         1.53268189e-07, -1.65792414e-07,  1.66425096e-07,  3.57291157e-07,\n",
       "        -4.72347529e-06,  3.73963030e-06,  4.80773315e-06,  4.99525822e-06,\n",
       "         2.17373827e-06,  8.80923920e-08, -6.22322659e-06,  2.16076910e-06,\n",
       "         3.23180757e-06,  2.32610569e-06,  1.30899070e-06, -7.57157110e-08,\n",
       "         5.77191940e-07, -9.97551979e-07,  8.94169034e-07,  3.56139230e-06,\n",
       "        -6.60420665e-06,  1.14543718e-06,  3.24972325e-06,  1.23916448e-06,\n",
       "         2.36919891e-06, -1.26996395e-06, -2.38275538e-06, -1.38625438e-07,\n",
       "         2.61007904e-06,  2.21631512e-06,  6.31635601e-07, -1.96053452e-06,\n",
       "        -1.29658895e-06,  3.19555738e-07, -2.19901221e-06,  6.63580568e-06,\n",
       "        -4.09463148e-08, -5.31294927e-06, -1.22709901e-06,  2.03442528e-06,\n",
       "         5.99618886e-07, -2.22483686e-06,  1.52548455e-06,  1.37322161e-07,\n",
       "         1.67855706e-06,  1.35754954e-06,  1.68320014e-06, -1.88560136e-06,\n",
       "         6.30625777e-07, -2.09472137e-06,  3.01899445e-06,  1.05428262e-06,\n",
       "        -6.43183853e-07,  1.11073214e-05,  2.22007293e-06,  1.31908416e-06,\n",
       "        -4.26841098e-07,  3.56636747e-07,  2.01170724e-06,  6.31681417e-07,\n",
       "         3.03678621e-06, -1.33714605e-06, -6.53363372e-07,  6.40833832e-06,\n",
       "        -2.10790063e-06, -5.70013390e-06,  2.00441299e-08,  2.07076027e-07,\n",
       "         5.15324427e-06, -2.15065870e-06,  7.25503514e-06,  2.20825268e-06,\n",
       "        -1.86256648e-06,  5.43914723e-07, -2.17793081e-06,  1.85466206e-07,\n",
       "        -1.09195014e-06, -4.68731741e-07,  3.32821037e-06,  5.17513990e-06,\n",
       "        -1.02406545e-06, -4.86963813e-07, -3.54658425e-07,  5.21060741e-08,\n",
       "         1.43339810e-06, -1.77663495e-07, -9.27648387e-07,  2.30030878e-06,\n",
       "        -4.54605834e-06, -4.77574758e-06, -1.43132115e-06, -2.31643116e-06,\n",
       "        -7.68544169e-07, -2.47983394e-07,  3.32224568e-06,  1.18322271e-06,\n",
       "         1.77955110e-06,  2.81984239e-06, -1.31039894e-06,  1.81902669e-06,\n",
       "         5.90515583e-07,  9.02941451e-07,  7.30617603e-06, -1.06600555e-06,\n",
       "        -2.94405709e-06, -3.42846397e-06,  1.79857216e-06,  4.45246133e-06,\n",
       "         4.39859264e-07, -1.54700012e-06,  3.34171409e-06,  1.74084516e-06,\n",
       "        -1.99281681e-06,  6.67963604e-07,  8.15065277e-07, -5.78168329e-06,\n",
       "         1.96547944e-06, -1.49311313e-06, -1.50706558e-06,  4.46658760e-06,\n",
       "        -1.47240132e-06,  3.45614035e-06, -1.34318861e-06, -9.68734753e-07,\n",
       "        -2.68803291e-07,  1.64395556e-06,  3.88501758e-06, -2.97030442e-06,\n",
       "        -5.37161554e-07,  5.38401878e-07,  1.76922299e-07, -5.54986684e-07,\n",
       "        -7.82404550e-06,  4.04971934e-06,  6.52801555e-06,  6.11442556e-06,\n",
       "         3.57331237e-06, -1.48981303e-06, -6.27202462e-06,  9.38531457e-07,\n",
       "         1.34267771e-06,  3.06520428e-06,  5.62688001e-07, -4.07161679e-06,\n",
       "        -6.35236859e-07, -8.52534299e-07,  8.38192591e-07,  6.39198788e-06,\n",
       "        -7.56079680e-06,  2.24954601e-06,  1.36111066e-06, -2.69055408e-06,\n",
       "         2.47259209e-06, -2.68070789e-06, -2.19827825e-06, -1.18209084e-06,\n",
       "         2.48147126e-06,  2.33658193e-06,  1.82484007e-06, -4.24260270e-06,\n",
       "         8.03228431e-07, -2.38816483e-07, -2.32750267e-06,  9.94713264e-06,\n",
       "         2.38149300e-06, -7.24835672e-06, -6.97927362e-07, -3.26981194e-06,\n",
       "         3.67651228e-06, -1.97345071e-06,  3.31499268e-06, -3.38124323e-07,\n",
       "         2.68249664e-06,  8.03565342e-07,  7.01121110e-07, -2.15152613e-06,\n",
       "        -2.81128450e-06, -1.05295817e-06,  3.10143105e-06,  2.97514953e-06,\n",
       "        -4.48429347e-07,  1.06453490e-05,  4.44066109e-06, -1.43311718e-06,\n",
       "        -1.47796720e-06,  9.44295607e-07,  1.96855581e-06, -2.04188086e-06,\n",
       "         1.92564016e-06, -4.50313837e-06, -1.27574049e-06,  6.07522679e-06,\n",
       "        -5.07206369e-06, -6.65355537e-06, -1.76668379e-06, -1.14263776e-06,\n",
       "         5.90015134e-06, -4.85322516e-06,  1.07324686e-05,  2.61695368e-06,\n",
       "         7.82767927e-07, -1.62839865e-06,  1.12298358e-06, -7.82889288e-07,\n",
       "         6.30014824e-07,  2.13462795e-06,  2.19474032e-06,  5.65078972e-06,\n",
       "         7.27280849e-07, -8.33000854e-07, -4.63417200e-06, -2.29422108e-06,\n",
       "         3.71600413e-06,  3.23026666e-06, -2.53781036e-06, -2.68105282e-06,\n",
       "        -4.29781130e-06, -4.10907978e-06, -1.99728402e-06,  7.08892685e-07,\n",
       "         1.16427793e-06, -6.81226879e-08,  3.88159924e-06,  5.64607888e-07,\n",
       "         1.68424663e-06,  2.76177229e-06, -7.47864135e-07, -3.11266945e-07,\n",
       "        -8.41633437e-07,  1.22093832e-06,  9.87964722e-06,  3.82960650e-07,\n",
       "        -3.15053194e-06, -4.55381041e-06, -9.01498538e-07,  4.29790725e-06,\n",
       "         2.23348411e-06, -1.10745532e-06,  2.97948236e-06,  1.43845114e-08,\n",
       "        -3.04330115e-07, -3.22838503e-08, -1.07091523e-06, -5.01825753e-06,\n",
       "         4.34579533e-06, -1.12422128e-06, -1.32210062e-06,  5.41632562e-06,\n",
       "        -1.32213381e-06,  7.26948656e-06, -8.34344974e-07,  2.57322040e-06,\n",
       "         1.15875628e-06,  2.94174811e-06,  5.14597423e-06, -2.15212526e-06],\n",
       "       dtype=float32),\n",
       " array([[-1.04461287e-04, -2.27579323e-04, -2.99930089e-05, ...,\n",
       "          1.99080631e-07,  2.00418413e-07,  1.95901649e-07],\n",
       "        [-1.35209004e-04,  2.85640526e-05,  4.48139908e-05, ...,\n",
       "         -1.02996488e-07, -1.03669734e-07, -1.01331501e-07],\n",
       "        [ 2.50519097e-05,  7.58191163e-08, -5.38271197e-05, ...,\n",
       "          1.49304960e-08,  1.49844830e-08,  1.46604897e-08],\n",
       "        ...,\n",
       "        [ 1.82322838e-04,  1.82267817e-04,  1.59925388e-04, ...,\n",
       "         -2.09390890e-07, -2.10738079e-07, -2.06012700e-07],\n",
       "        [-2.65916715e-05,  1.98283917e-04,  2.90460797e-04, ...,\n",
       "         -2.97095312e-07, -2.99013038e-07, -2.92327144e-07],\n",
       "        [ 1.29183158e-04,  3.19115316e-05,  1.34540314e-04, ...,\n",
       "         -2.79591632e-07, -2.81428839e-07, -2.75122375e-07]], dtype=float32),\n",
       " array([-0.7813121 , -1.0980024 , -0.9813515 , ...,  0.00201809,\n",
       "         0.00203134,  0.00198569], dtype=float32)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(grads, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h4>4. Apply the optimizer to the variables / gradients tuple.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Create the training TensorFlow Operation through our optimizer\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(train_op, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ltsm\"></a>\n",
    "<h2>LSTM</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "\n",
    "    def __init__(self, action_type):\n",
    "        ######################################\n",
    "        # Setting parameters for ease of use #\n",
    "        ######################################\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.hidden_size_l1 = hidden_size_l1\n",
    "        self.hidden_size_l2 = hidden_size_l2\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embeding_vector_size = embeding_vector_size\n",
    "        ###############################################################################\n",
    "        # Creating placeholders for our input data and expected outputs (target data) #\n",
    "        ###############################################################################\n",
    "        self._input_data = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]\n",
    "        self._targets = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]\n",
    "\n",
    "        lstm_cell_l1 = tf.contrib.rnn.BasicLSTMCell(self.hidden_size_l1, forget_bias=0.0)\n",
    "        lstm_cell_l2 = tf.contrib.rnn.BasicLSTMCell(self.hidden_size_l2, forget_bias=0.0)\n",
    "        \n",
    "        if action_type == \"is_training\" and keep_prob < 1:\n",
    "            lstm_cell_l1 = tf.contrib.rnn.DropoutWrapper(lstm_cell_l1, output_keep_prob=keep_prob)\n",
    "            lstm_cell_l2 = tf.contrib.rnn.DropoutWrapper(lstm_cell_l2, output_keep_prob=keep_prob)\n",
    "        \n",
    "        stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell_l1, lstm_cell_l2])\n",
    "\n",
    "        self._initial_state = stacked_lstm.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        \n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            # Create the embeddings for our input data. Size is hidden size.\n",
    "            embedding = tf.get_variable(\"embedding\", [vocab_size, self.embeding_vector_size])  #[10000x200]\n",
    "            # Define where to get the data for our embeddings from\n",
    "            inputs = tf.nn.embedding_lookup(embedding, self._input_data)\n",
    "\n",
    "        # Unless you changed keep_prob, this won't actually execute -- this is a dropout addition for our inputs\n",
    "        # This is an optimization of the input processing and is not needed at all\n",
    "        if action_type == \"is_training\" and keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, keep_prob)\n",
    "\n",
    "        \n",
    "        outputs, state = tf.nn.dynamic_rnn(stacked_lstm, inputs, initial_state=self._initial_state)\n",
    "        #########################################################################\n",
    "        # Creating a logistic unit to return the probability of the output word #\n",
    "        #########################################################################\n",
    "        output = tf.reshape(outputs, [-1, self.hidden_size_l2])\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [self.hidden_size_l2, vocab_size]) #[200x1000]\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) #[1x1000]\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])\n",
    "        prob = tf.nn.softmax(logits)\n",
    "        out_words = tf.argmax(prob, axis=2)\n",
    "        self._output_words = out_words\n",
    "        #########################################################################\n",
    "        # Defining the loss and cost functions for the model's learning to work #\n",
    "        #########################################################################\n",
    "            \n",
    "\n",
    "        # Use the contrib sequence loss and average over the batches\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(\n",
    "            logits,\n",
    "            self.targets,\n",
    "            tf.ones([batch_size, num_steps], dtype=tf.float32),\n",
    "            average_across_timesteps=False,\n",
    "            average_across_batch=True)\n",
    "    \n",
    "#         loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(self._targets, [-1])],\n",
    "#                                                       [tf.ones([batch_size * num_steps])])\n",
    "        self._cost = tf.reduce_sum(loss)\n",
    "\n",
    "        # Store the final state\n",
    "        self._final_state = state\n",
    "\n",
    "        #Everything after this point is relevant only for training\n",
    "        if action_type != \"is_training\":\n",
    "            return\n",
    "\n",
    "        #################################################\n",
    "        # Creating the Training Operation for our Model #\n",
    "        #################################################\n",
    "        # Create a variable for the learning rate\n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        # Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\n",
    "        tvars = tf.trainable_variables()\n",
    "        # Define the gradient clipping threshold\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self._cost, tvars), max_grad_norm)\n",
    "        # Create the gradient descent optimizer with our learning rate\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "        # Create the training TensorFlow Operation through our optimizer\n",
    "        self._train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    # Helper functions for our LSTM RNN class\n",
    "\n",
    "    # Assign the learning rate for this model\n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(tf.assign(self.lr, lr_value))\n",
    "\n",
    "    # Returns the input data for this model at a point in time\n",
    "    @property\n",
    "    def input_data(self):\n",
    "        return self._input_data\n",
    "\n",
    "\n",
    "    \n",
    "    # Returns the targets for this model at a point in time\n",
    "    @property\n",
    "    def targets(self):\n",
    "        return self._targets\n",
    "\n",
    "    # Returns the initial state for this model\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    # Returns the defined Cost\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    # Returns the final state for this model\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "    \n",
    "    # Returns the final output words for this model\n",
    "    @property\n",
    "    def final_output_words(self):\n",
    "        return self._output_words\n",
    "    \n",
    "    # Returns the current learning rate for this model\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    # Returns the training operation defined for this model\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "With that, the actual structure of our Recurrent Neural Network with Long Short-Term Memory is finished. What remains for us to do is to actually create the methods to run through time -- that is, the <code>run_epoch</code> method to be run at each epoch and a <code>main</code> script which ties all of this together.\n",
    "\n",
    "What our <code>run_epoch</code> method should do is take our input data and feed it to the relevant operations. This will return at the very least the current result for the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "##########################################################################################################################\n",
    "# run_one_epoch takes as parameters the current session, the model instance, the data to be fed, and the operation to be run #\n",
    "##########################################################################################################################\n",
    "def run_one_epoch(session, m, data, eval_op, verbose=False):\n",
    "\n",
    "    #Define the epoch size based on the length of the data, batch size and the number of steps\n",
    "    epoch_size = ((len(data) // m.batch_size) - 1) // m.num_steps\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "\n",
    "    state = session.run(m.initial_state)\n",
    "    \n",
    "    #For each step and data point\n",
    "    for step, (x, y) in enumerate(reader.ptb_iterator(data, m.batch_size, m.num_steps)):\n",
    "        \n",
    "        #Evaluate and return cost, state by running cost, final_state and the function passed as parameter\n",
    "        cost, state, out_words, _ = session.run([m.cost, m.final_state, m.final_output_words, eval_op],\n",
    "                                     {m.input_data: x,\n",
    "                                      m.targets: y,\n",
    "                                      m.initial_state: state})\n",
    "\n",
    "        #Add returned cost to costs (which keeps track of the total costs for this epoch)\n",
    "        costs += cost\n",
    "        \n",
    "        #Add number of steps to iteration counter\n",
    "        iters += m.num_steps\n",
    "\n",
    "        if verbose and step % (epoch_size // 10) == 10:\n",
    "            print(\"Itr %d of %d, perplexity: %.3f speed: %.0f wps\" % (step , epoch_size, np.exp(costs / iters), iters * m.batch_size / (time.time() - start_time)))\n",
    "\n",
    "    # Returns the Perplexity rating for us to keep track of how the model is evolving\n",
    "    return np.exp(costs / iters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, we create the <code>main</code> method to tie everything together. The code here reads the data from the directory, using the <code>reader</code> helper module, and then trains and evaluates the model on both a testing and a validating subset of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Reads the data and separates it into training data, validation data and testing data\n",
    "raw_data = reader.ptb_raw_data(data_dir)\n",
    "train_data, valid_data, test_data, _, _ = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Learning rate: 1.000\n",
      "Itr 10 of 774, perplexity: 4431.052 speed: 2008 wps\n",
      "Itr 87 of 774, perplexity: 1286.832 speed: 2064 wps\n",
      "Itr 164 of 774, perplexity: 987.177 speed: 2065 wps\n",
      "Itr 241 of 774, perplexity: 819.230 speed: 2060 wps\n",
      "Itr 318 of 774, perplexity: 724.194 speed: 2062 wps\n",
      "Itr 395 of 774, perplexity: 646.241 speed: 2064 wps\n",
      "Itr 472 of 774, perplexity: 585.237 speed: 2064 wps\n",
      "Itr 549 of 774, perplexity: 530.994 speed: 2067 wps\n",
      "Itr 626 of 774, perplexity: 487.721 speed: 2066 wps\n",
      "Itr 703 of 774, perplexity: 453.391 speed: 2065 wps\n",
      "Epoch 1 : Train Perplexity: 428.830\n",
      "Epoch 1 : Valid Perplexity: 233.221\n",
      "Epoch 2 : Learning rate: 1.000\n",
      "Itr 10 of 774, perplexity: 267.816 speed: 2013 wps\n",
      "Itr 87 of 774, perplexity: 235.621 speed: 2041 wps\n",
      "Itr 164 of 774, perplexity: 226.245 speed: 2053 wps\n",
      "Itr 241 of 774, perplexity: 216.714 speed: 2056 wps\n",
      "Itr 318 of 774, perplexity: 213.696 speed: 2058 wps\n",
      "Itr 395 of 774, perplexity: 207.841 speed: 2060 wps\n",
      "Itr 472 of 774, perplexity: 203.543 speed: 2060 wps\n",
      "Itr 549 of 774, perplexity: 196.982 speed: 2053 wps\n",
      "Itr 626 of 774, perplexity: 191.511 speed: 2052 wps\n",
      "Itr 703 of 774, perplexity: 187.389 speed: 2054 wps\n",
      "Epoch 2 : Train Perplexity: 184.682\n",
      "Epoch 2 : Valid Perplexity: 170.761\n",
      "Epoch 3 : Learning rate: 1.000\n",
      "Itr 10 of 774, perplexity: 184.550 speed: 2027 wps\n",
      "Itr 87 of 774, perplexity: 158.633 speed: 2046 wps\n",
      "Itr 164 of 774, perplexity: 154.669 speed: 2049 wps\n",
      "Itr 241 of 774, perplexity: 149.924 speed: 2052 wps\n",
      "Itr 318 of 774, perplexity: 149.896 speed: 2056 wps\n",
      "Itr 395 of 774, perplexity: 147.228 speed: 2059 wps\n",
      "Itr 472 of 774, perplexity: 145.662 speed: 2059 wps\n",
      "Itr 549 of 774, perplexity: 142.072 speed: 2061 wps\n",
      "Itr 626 of 774, perplexity: 139.297 speed: 2060 wps\n",
      "Itr 703 of 774, perplexity: 137.548 speed: 2059 wps\n",
      "Epoch 3 : Train Perplexity: 136.533\n",
      "Epoch 3 : Valid Perplexity: 149.906\n",
      "Epoch 4 : Learning rate: 1.000\n",
      "Itr 10 of 774, perplexity: 147.813 speed: 2047 wps\n",
      "Itr 87 of 774, perplexity: 126.872 speed: 2067 wps\n",
      "Itr 164 of 774, perplexity: 124.445 speed: 2066 wps\n",
      "Itr 241 of 774, perplexity: 120.988 speed: 2069 wps\n",
      "Itr 318 of 774, perplexity: 121.712 speed: 2068 wps\n",
      "Itr 395 of 774, perplexity: 119.846 speed: 2069 wps\n",
      "Itr 472 of 774, perplexity: 119.034 speed: 2070 wps\n",
      "Itr 549 of 774, perplexity: 116.366 speed: 2068 wps\n",
      "Itr 626 of 774, perplexity: 114.502 speed: 2070 wps\n",
      "Itr 703 of 774, perplexity: 113.482 speed: 2070 wps\n",
      "Epoch 4 : Train Perplexity: 112.994\n",
      "Epoch 4 : Valid Perplexity: 140.375\n",
      "Epoch 5 : Learning rate: 1.000\n",
      "Itr 10 of 774, perplexity: 126.640 speed: 2048 wps\n",
      "Itr 87 of 774, perplexity: 108.460 speed: 2068 wps\n",
      "Itr 164 of 774, perplexity: 106.733 speed: 2067 wps\n",
      "Itr 241 of 774, perplexity: 103.991 speed: 2069 wps\n",
      "Itr 318 of 774, perplexity: 104.881 speed: 2068 wps\n",
      "Itr 395 of 774, perplexity: 103.472 speed: 2070 wps\n",
      "Itr 472 of 774, perplexity: 102.987 speed: 2071 wps\n",
      "Itr 549 of 774, perplexity: 100.807 speed: 2069 wps\n",
      "Itr 626 of 774, perplexity: 99.370 speed: 2068 wps\n",
      "Itr 703 of 774, perplexity: 98.633 speed: 2068 wps\n",
      "Epoch 5 : Train Perplexity: 98.374\n",
      "Epoch 5 : Valid Perplexity: 134.814\n",
      "Epoch 6 : Learning rate: 0.500\n",
      "Itr 10 of 774, perplexity: 108.456 speed: 2008 wps\n",
      "Itr 87 of 774, perplexity: 93.236 speed: 2051 wps\n",
      "Itr 164 of 774, perplexity: 91.028 speed: 2052 wps\n",
      "Itr 241 of 774, perplexity: 87.887 speed: 2059 wps\n",
      "Itr 318 of 774, perplexity: 88.145 speed: 2067 wps\n",
      "Itr 395 of 774, perplexity: 86.385 speed: 2068 wps\n",
      "Itr 472 of 774, perplexity: 85.600 speed: 2070 wps\n",
      "Itr 549 of 774, perplexity: 83.267 speed: 2070 wps\n",
      "Itr 626 of 774, perplexity: 81.636 speed: 2072 wps\n",
      "Itr 703 of 774, perplexity: 80.650 speed: 2072 wps\n",
      "Epoch 6 : Train Perplexity: 80.097\n",
      "Epoch 6 : Valid Perplexity: 126.390\n",
      "Epoch 7 : Learning rate: 0.250\n",
      "Itr 10 of 774, perplexity: 94.758 speed: 2074 wps\n",
      "Itr 87 of 774, perplexity: 82.330 speed: 2078 wps\n",
      "Itr 164 of 774, perplexity: 80.611 speed: 2081 wps\n",
      "Itr 241 of 774, perplexity: 77.764 speed: 2079 wps\n",
      "Itr 318 of 774, perplexity: 77.984 speed: 2078 wps\n",
      "Itr 395 of 774, perplexity: 76.335 speed: 2076 wps\n",
      "Itr 472 of 774, perplexity: 75.561 speed: 2071 wps\n",
      "Itr 549 of 774, perplexity: 73.394 speed: 2070 wps\n",
      "Itr 626 of 774, perplexity: 71.826 speed: 2069 wps\n",
      "Itr 703 of 774, perplexity: 70.822 speed: 2070 wps\n",
      "Epoch 7 : Train Perplexity: 70.201\n",
      "Epoch 7 : Valid Perplexity: 124.064\n",
      "Epoch 8 : Learning rate: 0.125\n",
      "Itr 10 of 774, perplexity: 87.404 speed: 2039 wps\n",
      "Itr 87 of 774, perplexity: 76.336 speed: 2068 wps\n",
      "Itr 164 of 774, perplexity: 74.809 speed: 2069 wps\n",
      "Itr 241 of 774, perplexity: 72.161 speed: 2073 wps\n",
      "Itr 318 of 774, perplexity: 72.390 speed: 2073 wps\n",
      "Itr 395 of 774, perplexity: 70.818 speed: 2074 wps\n",
      "Itr 472 of 774, perplexity: 70.083 speed: 2073 wps\n",
      "Itr 549 of 774, perplexity: 68.022 speed: 2073 wps\n",
      "Itr 626 of 774, perplexity: 66.501 speed: 2072 wps\n",
      "Itr 703 of 774, perplexity: 65.508 speed: 2072 wps\n",
      "Epoch 8 : Train Perplexity: 64.873\n",
      "Epoch 8 : Valid Perplexity: 123.354\n",
      "Epoch 9 : Learning rate: 0.062\n",
      "Itr 10 of 774, perplexity: 83.453 speed: 2055 wps\n"
     ]
    }
   ],
   "source": [
    "# Initializes the Execution Graph and the Session\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "    initializer = tf.random_uniform_initializer(-init_scale, init_scale)\n",
    "    \n",
    "    # Instantiates the model for training\n",
    "    # tf.variable_scope add a prefix to the variables created with tf.get_variable\n",
    "    with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
    "        m = PTBModel(\"is_training\")\n",
    "        \n",
    "    # Reuses the trained parameters for the validation and testing models\n",
    "    # They are different instances but use the same variables for weights and biases, they just don't change when data is input\n",
    "    with tf.variable_scope(\"model\", reuse=True, initializer=initializer):\n",
    "        mvalid = PTBModel(\"is_validating\")\n",
    "        mtest = PTBModel(\"is_testing\")\n",
    "\n",
    "    #Initialize all variables\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for i in range(max_epoch):\n",
    "        # Define the decay for this epoch\n",
    "        lr_decay = decay ** max(i - max_epoch_decay_lr, 0.0)\n",
    "        \n",
    "        # Set the decayed learning rate as the learning rate for this epoch\n",
    "        m.assign_lr(session, learning_rate * lr_decay)\n",
    "\n",
    "        print(\"Epoch %d : Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
    "        \n",
    "        # Run the loop for this epoch in the training model\n",
    "        train_perplexity = run_one_epoch(session, m, train_data, m.train_op, verbose=True)\n",
    "        print(\"Epoch %d : Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "        \n",
    "        # Run the loop for this epoch in the validation model\n",
    "        valid_perplexity = run_one_epoch(session, mvalid, valid_data, tf.no_op())\n",
    "        print(\"Epoch %d : Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "    \n",
    "    # Run the loop in the testing model to see how effective was our training\n",
    "    test_perplexity = run_one_epoch(session, mtest, test_data, tf.no_op())\n",
    "    \n",
    "    print(\"Test Perplexity: %.3f\" % test_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "As you can see, the model's perplexity rating drops very quickly after a few iterations. As was elaborated before, <b>lower Perplexity means that the model is more certain about its prediction</b>. As such, we can be sure that this model is performing well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "-------"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
